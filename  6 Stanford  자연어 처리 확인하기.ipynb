{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=6, micro=1, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=1, releaselevel='final', serial=0)\n",
      "/Users/dahlmoon/anaconda/lib/python3.6/site-packages/nltk/VERSION\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(nltk.version_info)\n",
    "print(nltk.version_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스탠포드 처리 클래스 및 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StanfordNERTagger\n",
      "StanfordPOSTagger\n",
      "StanfordSegmenter\n",
      "StanfordTagger\n",
      "StanfordTokenizer\n",
      "stanford\n",
      "stanford_segmenter\n"
     ]
    }
   ],
   "source": [
    "for i in dir(nltk) :\n",
    "    if i.lower().startswith(\"stanford\") :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StanfordPOSTagger in module nltk.tag.stanford:\n",
      "\n",
      "class StanfordPOSTagger(StanfordTagger)\n",
      " |  A class for pos tagging with Stanford Tagger. The input is the paths to:\n",
      " |   - a model trained on training data\n",
      " |   - (optionally) the path to the stanford tagger jar file. If not specified here,\n",
      " |     then this jar file must be specified in the CLASSPATH envinroment variable.\n",
      " |   - (optionally) the encoding of the training data (default: UTF-8)\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |      >>> from nltk.tag import StanfordPOSTagger\n",
      " |      >>> st = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
      " |      >>> st.tag('What is the airspeed of an unladen swallow ?'.split())\n",
      " |      [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StanfordPOSTagger\n",
      " |      StanfordTagger\n",
      " |      nltk.tag.api.TaggerI\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from StanfordTagger:\n",
      " |  \n",
      " |  parse_output(self, text, sentences=None)\n",
      " |  \n",
      " |  tag(self, tokens)\n",
      " |      Determine the most appropriate tag sequence for the given\n",
      " |      token sequence, and return a corresponding list of tagged\n",
      " |      tokens.  A tagged token is encoded as a tuple ``(token, tag)``.\n",
      " |      \n",
      " |      :rtype: list(tuple(str, str))\n",
      " |  \n",
      " |  tag_sents(self, sentences)\n",
      " |      Apply ``self.tag()`` to each element of *sentences*.  I.e.:\n",
      " |      \n",
      " |          return [self.tag(sent) for sent in sentences]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.tag.api.TaggerI:\n",
      " |  \n",
      " |  evaluate(self, gold)\n",
      " |      Score the accuracy of the tagger against the gold standard.\n",
      " |      Strip the tags from the gold standard text, retag it using\n",
      " |      the tagger, then compute the accuracy score.\n",
      " |      \n",
      " |      :type gold: list(list(tuple(str, str)))\n",
      " |      :param gold: The list of tagged sentences to score the tagger on.\n",
      " |      :rtype: float\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.tag.api.TaggerI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.StanfordPOSTagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스탠포드에 대한 자연어 처리 추가\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "# Download the Stanford NLP tools\n",
    "wget http://nlp.stanford.edu/software/stanford-ner-2017-06-09.zip\n",
    "wget http://nlp.stanford.edu/software/stanford-postagger-full-2017-06-09.zip\n",
    "wget http://nlp.stanford.edu/software/stanford-parser-full-2017-06-09.zip\n",
    "# Extract the zip file.\n",
    "unzip stanford-ner-2017-06-09.zip \n",
    "unzip stanford-parser-full-2017-06-09.zip \n",
    "unzip stanford-postagger-full-2017-06-09.zip\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 클래스 패스 정의 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "CLASSPATH=/Users/dahlmoon/stanford-postagger/stanford-postagger.jar:/Users/dahlmoon/stanford-ner/stanford-ner.jar:/Users/dahlmoon/stanford-parser/stanford-parser.jar:/Users/dahlmoon/stanford-parser/stanford-parser-3.8.0-models.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제 필요한 자바 파일을 읽어오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dahlmoon//stanford-postagger/stanford-postagger.jar'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.tag.stanford\n",
    "\n",
    "nltk.tag.stanford.find_jar(\"stanford-postagger.jar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 품사 분리해서 tag 달기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('stanford', 'JJ'),\n",
       " ('postagger', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('nltk', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('python', 'NN'),\n",
       " ('users', 'NNS')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "english_postagger = nltk.StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "\n",
    "english_postagger.tag('this is stanford postagger in nltk for python users'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate\n",
      "parse_output\n",
      "tag\n",
      "tag_sents\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "for i in dir(StanfordPOSTagger) :\n",
    "    if not i.startswith(\"_\") :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What', 'WP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('airspeed', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('unladen', 'JJ'),\n",
       " ('swallow', 'VB'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "st = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "st.tag('What is the airspeed of an unladen swallow ?'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('stanford', 'JJ'),\n",
       " ('postagger', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('nltk', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('python', 'NN'),\n",
       " ('users', 'NNS')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "\n",
    "english_postagger = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "\n",
    "english_postagger.tag('this is stanford postagger in nltk for python users'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중국어 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', '这#PN'),\n",
       " ('', '是#VC'),\n",
       " ('', '在#P'),\n",
       " ('', 'Python#NN'),\n",
       " ('', '环境#NN'),\n",
       " ('', '中#LC'),\n",
       " ('', '使用#VV'),\n",
       " ('', '斯坦福#NR'),\n",
       " ('', '词性#JJ'),\n",
       " ('', '标#NN'),\n",
       " ('', '器#NN')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_postagger = StanfordPOSTagger('chinese-distsim.tagger', encoding='utf-8')\n",
    "\n",
    "chinese_postagger.tag('这 是 在 Python 环境 中 使用 斯坦福 词性 标 器'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stanford Named Entity Recognizer (NER) \n",
    "\n",
    "    스탠포드 네임드 엔티티를 인지하는 모듈 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate\n",
      "parse_output\n",
      "tag\n",
      "tag_sents\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "for i in dir(StanfordNERTagger) :\n",
    "    if not i.startswith(\"_\") :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rami', 'PERSON'),\n",
       " ('Eid', 'PERSON'),\n",
       " ('is', 'O'),\n",
       " ('studying', 'O'),\n",
       " ('at', 'O'),\n",
       " ('Stony', 'ORGANIZATION'),\n",
       " ('Brook', 'ORGANIZATION'),\n",
       " ('University', 'ORGANIZATION'),\n",
       " ('in', 'O'),\n",
       " ('NY', 'O')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "\n",
    "st.tag('Rami Eid is studying at Stony Brook University in NY'.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rami', 'PERSON'),\n",
       " ('Eid', 'PERSON'),\n",
       " ('is', 'O'),\n",
       " ('studying', 'O'),\n",
       " ('at', 'O'),\n",
       " ('Stony', 'ORGANIZATION'),\n",
       " ('Brook', 'ORGANIZATION'),\n",
       " ('University', 'ORGANIZATION'),\n",
       " ('in', 'O'),\n",
       " ('NY', 'O')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \n",
    "st.tag('Rami Eid is studying at Stony Brook University in NY'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rami', 'PERSON'),\n",
       " ('Eid', 'PERSON'),\n",
       " ('is', 'O'),\n",
       " ('studying', 'O'),\n",
       " ('at', 'O'),\n",
       " ('Stony', 'ORGANIZATION'),\n",
       " ('Brook', 'ORGANIZATION'),\n",
       " ('University', 'ORGANIZATION'),\n",
       " ('in', 'O'),\n",
       " ('NY', 'O')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "st.tag('Rami Eid is studying at Stony Brook University in NY'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('KOSPI', 'O'), ('oil', 'O'), ('price', 'O')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "st = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz') \n",
    "st.tag('KOSPI oil price'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parser  처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dahlmoon//stanford-parser/stanford-parser.jar'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.tag.stanford\n",
    "\n",
    "nltk.tag.stanford.find_jar('stanford-parser.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['this'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['english']), Tree('NN', ['parser']), Tree('NN', ['test'])])])])])]\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "english_parser = StanfordParser('/Users/dahlmoon//stanford-parser/stanford-parser.jar')\n",
    "b = english_parser.raw_parse_sents(('this is the english parser test', 'the parser is from stanford parser'))\n",
    "\n",
    "c = list(b)\n",
    "\n",
    "print(list(c[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dahlmoon//stanford-parser/stanford-parser-3.8.0-models.jar'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.tag.stanford\n",
    "\n",
    "nltk.tag.stanford.find_jar('stanford-parser-3.8.0-models.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['this'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP', [Tree('DT', ['the']), Tree('JJ', ['english']), Tree('NN', ['parser']), Tree('NN', ['test'])])])])])]\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "english_parser = StanfordParser('/Users/dahlmoon//stanford-parser/stanford-parser-3.8.0-models.jar')\n",
    "b = english_parser.raw_parse_sents(('this is the english parser test', 'the parser is from stanford parser'))\n",
    "\n",
    "c = list(b)\n",
    "\n",
    "print(list(c[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스탠포드 토큰화 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span_tokenize\n",
      "span_tokenize_sents\n",
      "tokenize\n",
      "tokenize_sents\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import StanfordTokenizer\n",
    "\n",
    "for i in dir(StanfordTokenizer) :\n",
    "    if not i.startswith(\"_\") :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import StanfordTokenizer\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.\"\n",
    "\n",
    "StanfordTokenizer().tokenize(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StanfordTokenizer in module nltk.tokenize.stanford:\n",
      "\n",
      "class StanfordTokenizer(nltk.tokenize.api.TokenizerI)\n",
      " |  Interface to the Stanford Tokenizer\n",
      " |  \n",
      " |  >>> from nltk.tokenize import StanfordTokenizer\n",
      " |  >>> s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.\"\n",
      " |  >>> StanfordTokenizer().tokenize(s)\n",
      " |  ['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
      " |  >>> s = \"The colour of the wall is blue.\"\n",
      " |  >>> StanfordTokenizer(options={\"americanize\": True}).tokenize(s)\n",
      " |  ['The', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StanfordTokenizer\n",
      " |      nltk.tokenize.api.TokenizerI\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, path_to_jar=None, encoding='utf8', options=None, verbose=False, java_options='-mx1000m')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  tokenize(self, s)\n",
      " |      Use stanford tokenizer's PTBTokenizer to tokenize multiple sentences.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.tokenize.api.TokenizerI:\n",
      " |  \n",
      " |  span_tokenize(self, s)\n",
      " |      Identify the tokens using integer offsets ``(start_i, end_i)``,\n",
      " |      where ``s[start_i:end_i]`` is the corresponding token.\n",
      " |      \n",
      " |      :rtype: iter(tuple(int, int))\n",
      " |  \n",
      " |  span_tokenize_sents(self, strings)\n",
      " |      Apply ``self.span_tokenize()`` to each element of ``strings``.  I.e.:\n",
      " |      \n",
      " |          return [self.span_tokenize(s) for s in strings]\n",
      " |      \n",
      " |      :rtype: iter(list(tuple(int, int)))\n",
      " |  \n",
      " |  tokenize_sents(self, strings)\n",
      " |      Apply ``self.tokenize()`` to each element of ``strings``.  I.e.:\n",
      " |      \n",
      " |          return [self.tokenize(s) for s in strings]\n",
      " |      \n",
      " |      :rtype: list(list(str))\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.tokenize.api.TokenizerI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StanfordTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"The colour of the wall is blue.\"\n",
    "\n",
    "StanfordTokenizer(options={\"americanize\": True}).tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
