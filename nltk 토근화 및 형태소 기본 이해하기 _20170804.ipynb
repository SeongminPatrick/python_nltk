{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLTK 패키지가 제공하는 주요 기능은 다음과 같다.\n",
    "\n",
    "    샘플 corpus 및 사전\n",
    "    토큰 생성(tokenizing)\n",
    "    형태소 분석(stemming/lemmatizing)\n",
    "    품사 태깅(part-of-speech tagging)\n",
    "    구문 분석(syntax parsing)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'nltk.tokenize' from '/Users/dahlmoon/anaconda/lib/python3.6/site-packages/nltk/tokenize/__init__.py'>\n",
      "['BlanklineTokenizer', 'LineTokenizer', 'MWETokenizer', 'PunktSentenceTokenizer', 'RegexpTokenizer', 'ReppTokenizer', 'SExprTokenizer', 'SpaceTokenizer', 'StanfordSegmenter', 'StanfordTokenizer', 'TabTokenizer', 'TextTilingTokenizer', 'ToktokTokenizer', 'TreebankWordTokenizer', 'TweetTokenizer', 'WhitespaceTokenizer', 'WordPunctTokenizer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_treebank_word_tokenizer', 'api', 'blankline_tokenize', 'casual', 'casual_tokenize', 'improved_close_quote_regex', 'improved_open_quote_regex', 'improved_punct_regex', 'line_tokenize', 'load', 'mwe', 'punkt', 're', 'regexp', 'regexp_span_tokenize', 'regexp_tokenize', 'repp', 'sent_tokenize', 'sexpr', 'sexpr_tokenize', 'simple', 'stanford', 'stanford_segmenter', 'string_span_tokenize', 'texttiling', 'toktok', 'treebank', 'util', 'word_tokenize', 'wordpunct_tokenize']\n"
     ]
    }
   ],
   "source": [
    "import nltk.tokenize \n",
    "\n",
    "print(nltk.tokenize)\n",
    "print(dir(nltk.tokenize))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토근화 하는 이유\n",
    "\n",
    "    긴 문자열을 처리하기 위해서는 작은 단위로 분리해야 한다.  일단 문장과 단어 등의 단위로 세분화할 수 있다.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  단어별로 토큰을 분리하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning, Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 토근화 이해하기 nltk.tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  단어별 토큰화 방식\n",
    "\n",
    "    TreebankWordTokenizer, PunktWordTokenizer, WordPunctTokenizer 방식\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어에 대한 토큰화 클래스 상속관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'nltk.tokenize.regexp.RegexpTokenizer'>,)\n",
      "(<class 'nltk.tokenize.regexp.RegexpTokenizer'>,)\n",
      "(<class 'nltk.tokenize.api.TokenizerI'>,)\n",
      "(<class 'object'>,)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.tokenize.WordPunctTokenizer.__bases__)\n",
    "print(nltk.tokenize.WhitespaceTokenizer.__bases__)\n",
    "print(nltk.tokenize.RegexpTokenizer.__bases__)\n",
    "print(nltk.tokenize.api.TokenizerI.__bases__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'nltk.tokenize.api.TokenizerI'>,)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.tokenize.TreebankWordTokenizer.__bases__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'nltk.tokenize.punkt.PunktBaseClass'>, <class 'nltk.tokenize.api.TokenizerI'>)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.tokenize.PunktSentenceTokenizer.__bases__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분리된 축약형으로 토근화 : TreebankWordTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', '’', 's', 'a', 'sent', 'tokenize', 'test.', 'this', 'is', 'sent', 'two.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', '’', 's', 'your', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method tokenize in module nltk.tokenize.treebank:\n",
      "\n",
      "tokenize(text, convert_parentheses=False, return_str=False) method of nltk.tokenize.treebank.TreebankWordTokenizer instance\n",
      "    Return a tokenized copy of *s*.\n",
      "    \n",
      "    :rtype: list of str\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분리된 문장부호로 토근화\n",
    "\n",
    "   사실, word_tokenize는 TreebankWordTokenizer에 의해 tokenize를 호출하는 래퍼 함수입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function word_tokenize in module nltk.tokenize:\n",
      "\n",
      "word_tokenize(text, language='english', preserve_line=False)\n",
      "    Return a tokenized copy of *text*,\n",
      "    using NLTK's recommended word tokenizer\n",
      "    (currently an improved :class:`.TreebankWordTokenizer`\n",
      "    along with :class:`.PunktSentenceTokenizer`\n",
      "    for the specified language).\n",
      "    \n",
      "    :param text: text to split into words\n",
      "    :param text: str\n",
      "    :param language: the model name in the Punkt corpus\n",
      "    :type language: str\n",
      "    :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\n",
      "    :type preserver_line: bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "help(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', '’', 's', 'a', 'sent', 'tokenize', 'test', '.', 'this', 'is', 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', '’', 's', 'your', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"Do not hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexpTokenizer\n",
    "\n",
    "    정규 표현식 토근화는 문자로 매칭하거나 스페이스로 매칭하는 것을 사용한다.\n",
    "    \n",
    "    regexp_tokenize 함수를 제공해서 결과를 리스트로 처리해 준다. \n",
    "    \n",
    "    하위 클래스로 WordPunctTokenizer 와 WhitespaceTokenizer가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "print(tokenizer.tokenize(\"Do not hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공백으로 정규표현식을 처리할 때는 gaps=True를 지정해야 실제 문자들이 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ']\n",
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\s]+\")\n",
    "print(tokenizer.tokenize(\"Do not hesitate to ask questions\"))\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\s]+\",gaps=True)\n",
    "print(tokenizer.tokenize(\"Do not hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function regexp_tokenize in module nltk.tokenize.regexp:\n",
      "\n",
      "regexp_tokenize(text, pattern, gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)\n",
      "    Return a tokenized copy of *text*.  See :class:`.RegexpTokenizer`\n",
      "    for descriptions of the arguments.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(regexp_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "print(regexp_tokenize(\"Do not hesitate to ask questions\",\"[\\w]+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ']\n",
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "print(regexp_tokenize(\"Do not hesitate to ask questions\",\"[\\s]+\"))\n",
    "\n",
    "print(regexp_tokenize(\"Do not hesitate to ask questions\",\"[\\s]+\",gaps=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  문장 부호를 완전히 새로운 토큰화 : WordPunctTokenizer\n",
    "\n",
    "   \\w+|[^\\w\\s]+ 이 정규 표현식을 이용해서 알파벳과 알파벳이 아닌 문자로 토큰화 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '''this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 's', 'a', 'sent', 'tokenize', 'test', 'this', 'is', 'sent', 'two', 'is', 'this', 'sent', 'three', 'sent', '4', 'is', 'cool', 'Now', 'it', 's', 'your', 'turn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "print(regexp_tokenize(text,\"\\w+\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 워드 다음에 붙여서 사용하는 마침표 쉼표 등을 구분하기 위해서는 [^\\w\\s]+로 확인한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['’', '.', '.', '?', '!', '’', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "print(regexp_tokenize(text,\"[^\\w\\s]+\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  실제 문자와 문장에 대한 기호를 처리해서 두가지 전부를 토근화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', '’', 's', 'a', 'sent', 'tokenize', 'test', '.', 'this', 'is', 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', '’', 's', 'your', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "print(regexp_tokenize(text,\"\\w+|[^\\w\\s]+\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', '’', 's', 'a', 'sent', 'tokenize', 'test', '.', 'this', 'is', 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', '’', 's', 'your', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(word_tokenize(\"Do not hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WhitespaceTokenizer \n",
    "\n",
    "    화이트스페잇에는 tab, space, newline 등이 있고 이를 기준으로 토큰화 처리를 한다.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this’s', 'a', 'sent', 'tokenize', 'test.', 'this', 'is', 'sent', 'two.', 'is', 'this', 'sent', 'three?', 'sent', '4', 'is', 'cool!', 'Now', 'it’s', 'your', 'turn.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰이 분리된 시작점과 끝점을 표시해서 출력한다 : span_tokenize 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 6), (7, 8), (9, 13), (14, 22), (23, 28), (29, 33), (34, 36), (37, 41), (42, 46), (47, 49), (50, 54), (55, 59), (60, 66), (67, 71), (72, 73), (74, 76), (77, 82), (83, 86), (87, 91), (92, 96), (97, 102)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(list(tokenizer.span_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaceTokenizer \n",
    "\n",
    "    파이썬 문자열의 split 메소드에서 인자로 \" \"를 받아서 처리하는 것과 동일하다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this’s', 'a', 'sent', 'tokenize', 'test.', 'this', 'is', 'sent', 'two.', 'is', 'this', 'sent', 'three?', 'sent', '4', 'is', 'cool!', 'Now', 'it’s', 'your', 'turn.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import SpaceTokenizer\n",
    "tokenizer = SpaceTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this’s', 'a', 'sent', 'tokenize', 'test.', 'this', 'is', 'sent', 'two.', 'is', 'this', 'sent', 'three?', 'sent', '4', 'is', 'cool!', 'Now', 'it’s', 'your', 'turn.']\n"
     ]
    }
   ],
   "source": [
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlanklineTokenizer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.']\n",
      "(0, 102)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "tokenizer = BlanklineTokenizer()\n",
    "\n",
    "test = '''\n",
    " this’s a sent tokenize test. \n",
    "this is sent two. \n",
    "is this sent three? \n",
    "\n",
    "\n",
    " sent 4 is cool! \n",
    "Now it’s your turn.\n",
    "\n",
    "'''\n",
    "\n",
    "print(tokenizer.tokenize(text))\n",
    "for i in tokenizer.span_tokenize(text) :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 단위로 토근화 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['this’s a sent tokenize test.', 'this is sent two.', 'is this sent three?', 'sent 4 is cool!', 'Now it’s your turn.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\"\"\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print(len(sent_tokenize_list))\n",
    "print(sent_tokenize_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 형태소 분석\n",
    "\n",
    "        형태소 분석이란 어근, 접두사/접미사, 품사(POS, part-of-speech) 등 다양한 언어적 속성의 구조를 파악하는 작업이다. \n",
    "        구체적으로는 다음과 같은 작업으로 나뉜다.\n",
    "\n",
    "        stemming (어근 추출)\n",
    "        lemmatizing (원형 복원)\n",
    "        POS tagging (품사 태깅)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "st.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shop'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "st.stem(\"shopping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "st = RegexpStemmer(\"ing\")\n",
    "st.stem(\"cooking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "print(lm.lemmatize(\"cooking\"))\n",
    "print(lm.lemmatize(\"cooking\", pos=\"v\"))\n",
    "print(lm.lemmatize(\"cookbooks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belief\n",
      "believ\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize(\"believes\"))\n",
    "print(LancasterStemmer().stem(\"believes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "\n",
    "POS(part-of-speech)는 품사를 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tag set\n",
    "\n",
    "    주요 품사에 대한 표시를 나타냄\n",
    "\n",
    "        AT : Article\n",
    "        NN : Noun\n",
    "        VB : Verb\n",
    "        JJ : Adjective\n",
    "        IN : Preposition\n",
    "        CD : Number\n",
    "        END : Sentence-ending punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), (',', ','), ('Arthur', 'NNP'), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', 'NNS'),\n",
       " ('Emma', 'NNP'),\n",
       " ('by', 'IN'),\n",
       " ('Jane', 'NNP'),\n",
       " ('Austen', 'NNP'),\n",
       " ('1816', 'CD'),\n",
       " (']', 'NNP'),\n",
       " ('VOLUME', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('CHAPTER', 'VBP'),\n",
       " ('I', 'PRP'),\n",
       " ('Emma', 'NNP'),\n",
       " ('Woodhouse', 'NNP'),\n",
       " (',', ','),\n",
       " ('handsome', 'NN'),\n",
       " (',', ','),\n",
       " ('clever', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('rich', 'JJ'),\n",
       " (',', ','),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "tagged_list = pos_tag(word_tokenize(emma_raw[:100]))\n",
    "tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Emma',\n",
       " 'by',\n",
       " 'Jane',\n",
       " 'Austen',\n",
       " '1816',\n",
       " ']',\n",
       " 'VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'and',\n",
       " 'rich',\n",
       " ',',\n",
       " 'with',\n",
       " 'a']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "untag(tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
