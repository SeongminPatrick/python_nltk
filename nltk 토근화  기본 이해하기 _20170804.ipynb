{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLTK 패키지가 제공하는 주요 기능은 다음과 같다.\n",
    "\n",
    "    샘플 corpus 및 사전\n",
    "    토큰 생성(tokenizing)\n",
    "    형태소 분석(stemming/lemmatizing)\n",
    "    품사 태깅(part-of-speech tagging)\n",
    "    구문 분석(syntax parsing)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Accessing corpora          nltk.corpus                   Standardized interfaces to corpora and lexicons\n",
    "\n",
    "    String processing          nltk.tokenize, nltk.stem      Tokenizers, sentence tokenizers, stemmers\n",
    "\n",
    "    Collocation discovery      nltk.collocations             t-test, chi-squared, point-wise mutual information\n",
    "\n",
    "    Part-of-speech tagging     nltk.tag                      n-gram, backoff, Brill, HMM, TnT\n",
    "\n",
    "    Classification             nltk.classify, nltk.cluster   Decision tree, maximum entropy, naive Bayes, EM, k-means\n",
    "\n",
    "    Chunking                   nltk.chunk                    Regular expression, n-gram, named entity\n",
    "\n",
    "    Parsing                    nltk.parse                    Chart, feature-based, unification, probabilistic,         dependency\n",
    "\n",
    "    Semantic interpretation    nltk.sem, nltk.inference      Lambda calculus, first-order logic, model checking\n",
    "\n",
    "    Evaluation metrics         nltk.metrics                  Precision, recall, agreement coefficients\n",
    "\n",
    "    Probability and estimation nltk.probability              Frequency distributions, smoothed probability distributions\n",
    "\n",
    "    Applications               nltk.app, nltk.chat           Graphical concordancer, parsers, WordNet browser, chatbots\n",
    "\n",
    "    Linguistic fieldwork       nltk.toolbox                  Manipulate data in SIL Toolbox format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토근화 모듈 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'nltk.tokenize' from '/Users/dahlmoon/anaconda/lib/python3.6/site-packages/nltk/tokenize/__init__.py'>\n",
      "['BlanklineTokenizer', 'LineTokenizer', 'MWETokenizer', 'PunktSentenceTokenizer', 'RegexpTokenizer', 'ReppTokenizer', 'SExprTokenizer', 'SpaceTokenizer', 'StanfordSegmenter', 'StanfordTokenizer', 'TabTokenizer', 'TextTilingTokenizer', 'ToktokTokenizer', 'TreebankWordTokenizer', 'TweetTokenizer', 'WhitespaceTokenizer', 'WordPunctTokenizer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_treebank_word_tokenizer', 'api', 'blankline_tokenize', 'casual', 'casual_tokenize', 'improved_close_quote_regex', 'improved_open_quote_regex', 'improved_punct_regex', 'line_tokenize', 'load', 'mwe', 'punkt', 're', 'regexp', 'regexp_span_tokenize', 'regexp_tokenize', 'repp', 'sent_tokenize', 'sexpr', 'sexpr_tokenize', 'simple', 'stanford', 'stanford_segmenter', 'string_span_tokenize', 'texttiling', 'toktok', 'treebank', 'util', 'word_tokenize', 'wordpunct_tokenize']\n"
     ]
    }
   ],
   "source": [
    "import nltk.tokenize \n",
    "\n",
    "print(nltk.tokenize)\n",
    "print(dir(nltk.tokenize))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토근화 하는 이유\n",
    "\n",
    "    긴 문자열을 처리하기 위해서는 작은 단위로 분리해야 한다.  일단 문장과 단어 등의 단위로 세분화할 수 있다.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  단어별로 토큰을 분리하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', ',', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning, Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 토근화 이해하기 nltk.tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  단어별 토큰화 방식\n",
    "\n",
    "    TreebankWordTokenizer, PunktWordTokenizer, WordPunctTokenizer 방식\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어에 대한 토큰화 클래스 상속관계 : RegexpTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'nltk.tokenize.regexp.RegexpTokenizer'>,)\n",
      "(<class 'nltk.tokenize.regexp.RegexpTokenizer'>,)\n",
      "(<class 'nltk.tokenize.api.TokenizerI'>,)\n",
      "(<class 'object'>,)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.tokenize.WordPunctTokenizer.__bases__)\n",
    "print(nltk.tokenize.WhitespaceTokenizer.__bases__)\n",
    "print(nltk.tokenize.RegexpTokenizer.__bases__)\n",
    "print(nltk.tokenize.api.TokenizerI.__bases__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'nltk.tokenize.api.TokenizerI'>,)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.tokenize.TreebankWordTokenizer.__bases__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'object'>,)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.tokenize.TweetTokenizer.__bases__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 분리된 축약형으로 토근화 : TreebankWordTokenizer\n",
    "\n",
    "    축약형에 대한 단어를 분리할 때  don't 일 경우는 do 와 n't으로 분리\n",
    "    this's는 this, ' , s로 분리한다.\n",
    "    \n",
    "    그리고 문장에서 마지막 마침표는 별도로 분리한다.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', '’', 's', 'a', 'sent', 'tokenize', 'test.', 'this', 'is', 'sent', 'two.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', '’', 's', 'your', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분리된 문장부호로 토근화\n",
    "\n",
    "   사실, word_tokenize는 TreebankWordTokenizer에 의해 tokenize를 호출하는 래퍼 함수입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function word_tokenize in module nltk.tokenize:\n",
      "\n",
      "word_tokenize(text, language='english', preserve_line=False)\n",
      "    Return a tokenized copy of *text*,\n",
      "    using NLTK's recommended word tokenizer\n",
      "    (currently an improved :class:`.TreebankWordTokenizer`\n",
      "    along with :class:`.PunktSentenceTokenizer`\n",
      "    for the specified language).\n",
      "    \n",
      "    :param text: text to split into words\n",
      "    :param text: str\n",
      "    :param language: the model name in the Punkt corpus\n",
      "    :type language: str\n",
      "    :param preserve_line: An option to keep the preserve the sentence and not sentence tokenize it.\n",
      "    :type preserver_line: bool\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "help(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', '’', 's', 'a', 'sent', 'tokenize', 'test', '.', 'this', 'is', 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', '’', 's', 'your', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"Do not hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexpTokenizer\n",
    "\n",
    "    정규 표현식 토근화는 문자로 매칭하거나 스페이스로 매칭하는 것을 사용한다.\n",
    "    \n",
    "    regexp_tokenize 함수를 제공해서 결과를 리스트로 처리해 준다. \n",
    "    \n",
    "    하위 클래스로 WordPunctTokenizer 와 WhitespaceTokenizer가 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 워드 문서로만 분리하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "\n",
    "print(tokenizer.tokenize(\"Do not hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공백[\\s] 으로 정규표현식을 처리할 때는 gaps=True를 지정해야 실제 문자들이 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ']\n",
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\s]+\")\n",
    "print(tokenizer.tokenize(\"Do not hesitate to ask questions\"))\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\s]+\",gaps=True)\n",
    "print(tokenizer.tokenize(\"Do not hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수를 이용해서 분리할 경우에는 텍스트와 패턴을 넣어야 한다. 특히 스페이스일 경우는 gaps=True를 사용해야 문자를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function regexp_tokenize in module nltk.tokenize.regexp:\n",
      "\n",
      "regexp_tokenize(text, pattern, gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)\n",
      "    Return a tokenized copy of *text*.  See :class:`.RegexpTokenizer`\n",
      "    for descriptions of the arguments.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(regexp_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "print(regexp_tokenize(\"Do not hesitate to ask questions\",\"[\\w]+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ']\n",
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "print(regexp_tokenize(\"Do not hesitate to ask questions\",\"[\\s]+\"))\n",
    "\n",
    "print(regexp_tokenize(\"Do not hesitate to ask questions\",\"[\\s]+\",gaps=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  문장 부호를 완전히 새로운 토큰화 : WordPunctTokenizer\n",
    "\n",
    "   \\w+|[^\\w\\s]+ 이 정규 표현식을 이용해서 알파벳과 알파벳이 아닌 문자로 토큰화 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '''this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 's', 'a', 'sent', 'tokenize', 'test', 'this', 'is', 'sent', 'two', 'is', 'this', 'sent', 'three', 'sent', '4', 'is', 'cool', 'Now', 'it', 's', 'your', 'turn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "print(regexp_tokenize(text,\"\\w+\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 워드 다음에 붙여서 사용하는 마침표 쉼표 등을 구분하기 위해서는 [^\\w\\s]+로 확인한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['’', '.', '.', '?', '!', '’', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "print(regexp_tokenize(text,\"[^\\w\\s]+\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  실제 문자와 문장에 대한 기호를 처리해서 두가지 전부를 토근화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', '’', 's', 'a', 'sent', 'tokenize', 'test', '.', 'this', 'is', 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', '’', 's', 'your', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "print(regexp_tokenize(text,\"\\w+|[^\\w\\s]+\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', '’', 's', 'a', 'sent', 'tokenize', 'test', '.', 'this', 'is', 'sent', 'two', '.', 'is', 'this', 'sent', 'three', '?', 'sent', '4', 'is', 'cool', '!', 'Now', 'it', '’', 's', 'your', 'turn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(word_tokenize(\"Do not hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WhitespaceTokenizer \n",
    "\n",
    "    화이트스페잇에는 tab, space, newline 등이 있고 이를 기준으로 토큰화 처리를 한다.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this’s', 'a', 'sent', 'tokenize', 'test.', 'this', 'is', 'sent', 'two.', 'is', 'this', 'sent', 'three?', 'sent', '4', 'is', 'cool!', 'Now', 'it’s', 'your', 'turn.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토큰이 분리된 시작점과 끝점을 표시해서 출력한다 : span_tokenize 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 6), (7, 8), (9, 13), (14, 22), (23, 28), (29, 33), (34, 36), (37, 41), (42, 46), (47, 49), (50, 54), (55, 59), (60, 66), (67, 71), (72, 73), (74, 76), (77, 82), (83, 86), (87, 91), (92, 96), (97, 102)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(list(tokenizer.span_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaceTokenizer  \n",
    "\n",
    "    파이썬 문자열의 split 메소드에서 인자로 \" \"를 받아서 처리하는 것과 동일하다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this’s', 'a', 'sent', 'tokenize', 'test.', 'this', 'is', 'sent', 'two.', 'is', 'this', 'sent', 'three?', 'sent', '4', 'is', 'cool!', 'Now', 'it’s', 'your', 'turn.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import SpaceTokenizer\n",
    "tokenizer = SpaceTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split 메소드에 블랭크로 분리하는 것도 동일한 결과를 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this’s', 'a', 'sent', 'tokenize', 'test.', 'this', 'is', 'sent', 'two.', 'is', 'this', 'sent', 'three?', 'sent', '4', 'is', 'cool!', 'Now', 'it’s', 'your', 'turn.']\n"
     ]
    }
   ],
   "source": [
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2  문장 단위로 토근화 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlanklineTokenizer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.']\n",
      "(0, 102)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "tokenizer = BlanklineTokenizer()\n",
    "\n",
    "test = '''\n",
    " this’s a sent tokenize test. \n",
    "this is sent two. \n",
    "is this sent three? \n",
    "\n",
    "\n",
    " sent 4 is cool! \n",
    "Now it’s your turn.\n",
    "\n",
    "'''\n",
    "\n",
    "print(tokenizer.tokenize(text))\n",
    "for i in tokenizer.span_tokenize(text) :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sent_tokenize 함수를 이용해서 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['this’s a sent tokenize test.', 'this is sent two.', 'is this sent three?', 'sent 4 is cool!', 'Now it’s your turn.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\"\"\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print(len(sent_tokenize_list))\n",
    "print(sent_tokenize_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 정규화 : 문장 부호 없애기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.tokenize as tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^_\\`\\{\\|\\}\\~\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(re.escape(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['IT', 'is', 'a', 'pleasant', 'evening', '.'], ['Guests', ',', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty', '.']]\n"
     ]
    }
   ],
   "source": [
    "text1 = [\"IT is a pleasant evening.\", \"Guests, who came from US arrived at the venue\", \"Food was tasty.\"]\n",
    "\n",
    "tokenized_doc = [tok.word_tokenize(doc) for doc in text1]\n",
    "\n",
    "print(tokenized_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]')\n"
     ]
    }
   ],
   "source": [
    "x = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x에 등록된 구둣점을 빈공칸으로 변경한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['IT', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "b = []\n",
    "for i in tokenized_doc :\n",
    "    a = [x.sub(\" \", token) for token in i]\n",
    "    a.remove(\" \")\n",
    "    b.append(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 처리  : stop words\n",
    "\n",
    "    문장의 전체적인 의미에 크게 기여하지 않는 단어를 필터링해서 처리한다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어를 지원하는 언어들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어에 대한 불용어 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'himself', 'he', 'i', 'don', 'in', 'off', 'down', 'its', 'were', 'mightn', 'theirs', 'is', 'on', 'couldn', 'while', 'here', 'll', 'm', 'few', 'again', 'can', 'under', 'shouldn', 'had', 'more', 'so', 'for', 'wouldn', 'your', 'not', 'should', 're', 'been', 'what', 'o', 'or', 'each', 'isn', 'doing', 'these', 'd', 'hadn', 's', 'our', 'hasn', 'into', 've', 'does', 'themselves', 'will', 'am', 'same', 'both', 'ain', 'those', 'herself', 'no', 'you', 'there', 'if', 'nor', 'because', 'their', 'didn', 'was', 'but', 'than', 'they', 'y', 'before', 'any', 'be', 'an', 'at', 'doesn', 'very', 'all', 'from', 'needn', 'ourselves', 'her', 'we', 'with', 'during', 'some', 'only', 'has', 'out', 'by', 'until', 'against', 't', 'about', 'of', 'it', 'above', 'mustn', 'and', 'my', 'why', 'yourselves', 'are', 'such', 'yours', 'them', 'how', 'own', 'having', 'this', 'the', 'up', 'weren', 'other', 'him', 'a', 'too', 'hers', 'me', 'wasn', 'now', 'further', 'when', 'where', 'won', 'yourself', 'below', 'just', 'shan', 'his', 'have', 'between', 'aren', 'whom', 'which', 'over', 'after', 'ma', 'itself', 'do', 'once', 'as', 'then', 'myself', 'that', 'ours', 'who', 'being', 'to', 'most', 'she', 'haven', 'through', 'did'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = [['IT', 'is', 'a', 'pleasant', 'evening'],\n",
    " ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'],\n",
    " ['Food', 'was', 'tasty']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "a\n",
      "who\n",
      "from\n",
      "at\n",
      "the\n",
      "was\n",
      "[['IT', 'pleasant', 'evening'], ['Guests', 'came', 'US', 'arrived', 'venue'], ['Food', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "d = []\n",
    "for i in b :\n",
    "    c = []\n",
    "    for  j in range(len(i)) :\n",
    "        if i[j] in stops :\n",
    "            print(i[j])\n",
    "        else :\n",
    "            c.append(i[j])\n",
    "    d.append(c)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1720901"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  nltk.corpus as corpus\n",
    "\n",
    "len(corpus.reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145735"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.inaugural.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
